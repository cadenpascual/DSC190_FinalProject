{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "merged_title",
   "metadata": {},
   "source": [
    "# SET Data Analysis\n",
    "\n",
    "This notebook contains comprehensive analysis of DSC SET (Student Evaluation of Teaching) data, including data cleaning, exploratory data analysis, and statistical modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b81310",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical and machine learning libraries\n",
    "from scipy.stats import ttest_ind, f_oneway, chi2_contingency, pearsonr, spearmanr\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c204a322",
   "metadata": {},
   "source": [
    "## Data Loading/Cleaning\n",
    "In this section we load and merge the data for EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54083986",
   "metadata": {},
   "source": [
    "*Load and clean SETs Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041c7a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sets into dataframe\n",
    "dsc_sets = pd.read_csv('datasets/dsc_sets.csv')\n",
    "\n",
    "# merge duplicate columns\n",
    "dsc_sets[\"Enrolled Resp Rate\"] = dsc_sets[\"Enrolled/ Resp Rate\"].fillna(dsc_sets[\"Enrolled/  Resp Rate\"])\n",
    "dsc_sets = dsc_sets.drop(columns=[\"Enrolled/ Resp Rate\", \"Enrolled/  Resp Rate\"])\n",
    "\n",
    "# rename columns for easier access\n",
    "dsc_sets.columns = dsc_sets.columns.str.replace('*', '', regex=False)\n",
    "dsc_sets.columns = dsc_sets.columns.str.replace(' ', '_')\n",
    "dsc_sets['Term'] = dsc_sets['Term'].str.lower()\n",
    "\n",
    "\n",
    "# Extract the number before the parentheses\n",
    "dsc_sets['Enrolled_Resp_Number'] = dsc_sets['Enrolled_Resp_Rate'].str.extract(r'(\\d+)\\s*\\(')[0].astype(int)\n",
    "# Extract the percentage inside parentheses\n",
    "dsc_sets['Enrolled_Resp_Pct'] = dsc_sets['Enrolled_Resp_Rate'].str.extract(r'\\(([\\d\\.]+)%\\)')[0].astype(float)\n",
    "\n",
    "# remove Letter from Avg Grade Received\n",
    "dsc_sets[\"Avg_Grade_Received\"] = (\n",
    "    dsc_sets[\"Avg_Grade_Received\"]\n",
    "    .str.extract(r'(\\d+\\.\\d+|\\d+)')   # extract first number\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "# delete unnecessary columns\n",
    "dsc_sets = dsc_sets.drop(columns=[\"Course\", \"Enrolled_Resp_Rate\"])\n",
    "# remove dsc96 course (doesn't have grades)\n",
    "dsc_sets = dsc_sets[dsc_sets['course_title'] != 'dsc95']\n",
    "\n",
    "dsc_sets = dsc_sets.dropna(subset=['Avg_Grade_Received'])\n",
    "dsc_sets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20be746e",
   "metadata": {},
   "source": [
    "*Merge SETs data with Utilization Rate from WebReg Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfce538",
   "metadata": {},
   "outputs": [],
   "source": [
    "webreg_data = pd.read_csv('webreg_data/results/webreg_processed_data.csv')\n",
    "webreg_data.head()\n",
    "merged_data = webreg_data[['course', 'quarter', 'utilization_rate']]\n",
    "merged_data = merged_data.copy()\n",
    "\n",
    "#drop underscore from course column to match dsc_sets\n",
    "merged_data['course'] = merged_data['course'].str.replace('_', '').str.lower()\n",
    "merged_data = merged_data.rename(columns={'course': 'course_title', 'quarter': 'Term'})\n",
    "# merge datasets on course title and term\n",
    "final_data = pd.merge(dsc_sets, merged_data, on=['course_title', 'Term'], how='inner')\n",
    "cols = ['course_title'] + [c for c in final_data.columns if c != 'course_title']\n",
    "final_data = final_data[cols]\n",
    "\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d92fd9",
   "metadata": {},
   "source": [
    "## EDA of SETs + Utilization Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d1c3b7",
   "metadata": {},
   "source": [
    "### Correlation of Numeric Course Metrics\n",
    "This visualization shows the pairwise linear correlations among key numeric course metrics in the dataset, focusing on the lower triangle of the correlation matrix to avoid redundancy.\n",
    "\n",
    "The variables included are:\n",
    "* Avg_Grade_Received – average grade students earned in the course\n",
    "* Avg_Hours_Worked – average weekly hours students reported spending on the course\n",
    "* Learning_Average – students’ evaluation of how much they learned\n",
    "* Structure_Average – students’ evaluation of the course’s organization and clarity\n",
    "* Environment_Average – students’ evaluation of the classroom and learning environment\n",
    "* Enrolled_Resp_Number – number of students enrolled/responded\n",
    "* Enrolled_Resp_Pct – enrollment/response percentage\n",
    "\n",
    "`Structure_Average`, `Learning_Average`, and `Environment_Average` exhibit strong positive correlations, indicating consistent student perceptions across these aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25918e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['Avg_Grade_Received', 'Avg_Hours_Worked',\n",
    "                'Learning_Average', 'Structure_Average', \n",
    "                'Environment_Average', 'Enrolled_Resp_Number',\n",
    "                'Enrolled_Resp_Pct']\n",
    "numeric_df = final_data[numeric_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a79b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = numeric_df.corr()\n",
    "plt.figure(figsize=(8,6))  # set figure size\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    annot=True,       # show correlation values\n",
    "    fmt=\".2f\",        # format numbers\n",
    "    cmap='coolwarm',  # color map\n",
    "    vmin=-1, vmax=1,   # fix color scale\n",
    "    mask=mask      # hide diagonal\n",
    ")\n",
    "plt.title(\"Correlation Between Numeric Metrics\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762e0c1e",
   "metadata": {},
   "source": [
    "### Test 1: Test for Multi-Colinearity\n",
    "\n",
    "**Purpose**: Determine if numeric predictors are highly correlated, which can inflate regression coefficients and make them unstable.\n",
    "\n",
    "**Testing**: \n",
    "* **Coefficient**: Change in `utilization_rate` for a one-unit increase in predictor.\n",
    "* **p-value**: If predictor's effect is significantly associated with `utilization_rate` (is if \\< 0.05)\n",
    "* **$R^2$**: Proportion of variance in `utilization_rate` explained by the predictor alone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5f5115",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numeric_df.dropna() \n",
    "X = sm.add_constant(X) \n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e093b98c",
   "metadata": {},
   "source": [
    "### Test 2: Simple Linear Regression of Numeric Predictors on Utilization Rate\n",
    "\n",
    "**Purpose**: Find if any numeric variables predict `utilization_rate` in a univariate model.\n",
    "\n",
    "**Testing**: \n",
    "* **Coefficient**: Change in `utilization_rate` for a one-unit increase in predictor.\n",
    "* **p-value**: If predictor's effect is significantly associated with `utilization_rate` (is if \\< 0.05)\n",
    "* **$R^2$**: Proportion of variance in `utilization_rate` explained by the predictor alone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53387519",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    X = final_data[[col]]\n",
    "    Y = final_data['utilization_rate']\n",
    "    \n",
    "    # Add constant for intercept\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    # Fit model\n",
    "    model = sm.OLS(Y, X).fit()\n",
    "    \n",
    "    # Save results\n",
    "    results.append({\n",
    "        'Predictor': col,\n",
    "        'Coefficient': model.params[col],\n",
    "        'Intercept': model.params['const'],\n",
    "        'R_squared': model.rsquared,\n",
    "        'p_value': model.pvalues[col]\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easy viewing\n",
    "regression_results = pd.DataFrame(results)\n",
    "regression_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2e4f77",
   "metadata": {},
   "source": [
    "**Interpretation**: \n",
    "* **Coefficient**: Change in `utilization_rate` for a one-unit increase in predictor.\n",
    "* **p-value**: If predictor's effect is significantly associated with `utilization_rate` (is if \\< 0.05)\n",
    "* **$R^2$**: Proportion of variance in `utilization_rate` explained by the predictor alone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separator",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Advanced Analysis: Difficulty Prediction and Statistical Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b81310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical and machine learning libraries\n",
    "from scipy.stats import ttest_ind, f_oneway, chi2_contingency, pearsonr, spearmanr\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfce538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SET data (ONLY data source - no WebReg)\n",
    "dsc_sets = pd.read_csv('datasets/dsc_sets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ef10b5",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning and Feature Engineering\n",
    "\n",
    "We need to:\n",
    "1. Extract course numbers from course titles\n",
    "2. Standardize term formats\n",
    "3. Clean and extract numeric values from grade fields\n",
    "4. Aggregate SET data by course and quarter\n",
    "5. Create derived features for analysis\n",
    "\n",
    "**Note:** This analysis uses ONLY SET data. No WebReg data is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7dbdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to extract course number from course title\n",
    "def extract_course_number(course_title):\n",
    "    \"\"\"Extract numeric course number from course title (e.g., 'dsc100' -> 100)\"\"\"\n",
    "    if pd.isna(course_title):\n",
    "        return None\n",
    "    import re\n",
    "    match = re.search(r'dsc(\\d+)', str(course_title).lower())\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "# Helper function to standardize term format\n",
    "def standardize_term(term):\n",
    "    \"\"\"Convert term format to match webreg format (e.g., 'FA24' -> 'fa24')\"\"\"\n",
    "    if pd.isna(term):\n",
    "        return None\n",
    "    term_str = str(term).upper()\n",
    "    term_map = {\n",
    "        'FA24': 'fa24', 'WI25': 'wi25', 'SP25': 'sp25',\n",
    "        'FALL 2024': 'fa24', 'WINTER 2025': 'wi25', 'SPRING 2025': 'sp25',\n",
    "        'FA 2024': 'fa24', 'WI 2025': 'wi25', 'SP 2025': 'sp25'\n",
    "    }\n",
    "    if term_str in term_map:\n",
    "        return term_map[term_str]\n",
    "    import re\n",
    "    match = re.search(r'(FA|WI|SP|FALL|WINTER|SPRING)\\s*(\\d{2,4})', term_str)\n",
    "    if match:\n",
    "        season = match.group(1)[:2].upper()\n",
    "        year = match.group(2)[-2:]\n",
    "        if season.startswith('FA'):\n",
    "            return f'fa{year}'\n",
    "        elif season.startswith('WI'):\n",
    "            return f'wi{year}'\n",
    "        elif season.startswith('SP'):\n",
    "            return f'sp{year}'\n",
    "    return None\n",
    "\n",
    "# Clean SET data (ONLY data source)\n",
    "\n",
    "# First, clean column names (before creating new columns)\n",
    "dsc_sets.columns = dsc_sets.columns.str.replace('*', '', regex=False)\n",
    "dsc_sets.columns = dsc_sets.columns.str.replace(' ', '_')\n",
    "dsc_sets.columns = dsc_sets.columns.str.replace('/', '_')\n",
    "\n",
    "# Handle Enrolled Resp Rate columns (merge duplicates)\n",
    "if \"Enrolled_Resp_Rate\" in dsc_sets.columns and \"Enrolled__Resp_Rate\" in dsc_sets.columns:\n",
    "    dsc_sets[\"Enrolled_Resp_Rate\"] = dsc_sets[\"Enrolled_Resp_Rate\"].fillna(dsc_sets[\"Enrolled__Resp_Rate\"])\n",
    "    dsc_sets = dsc_sets.drop(columns=[\"Enrolled__Resp_Rate\"], errors='ignore')\n",
    "elif \"Enrolled_Resp_Rate\" in dsc_sets.columns:\n",
    "    pass  # Already exists\n",
    "elif \"Enrolled__Resp_Rate\" in dsc_sets.columns:\n",
    "    dsc_sets[\"Enrolled_Resp_Rate\"] = dsc_sets[\"Enrolled__Resp_Rate\"]\n",
    "    dsc_sets = dsc_sets.drop(columns=[\"Enrolled__Resp_Rate\"], errors='ignore')\n",
    "\n",
    "# Drop unnecessary columns first\n",
    "dsc_sets = dsc_sets.drop(columns=[\"Course\"], errors='ignore')\n",
    "\n",
    "# Handle duplicate columns BEFORE converting to numeric\n",
    "# Find all duplicate column names\n",
    "all_cols = list(dsc_sets.columns)\n",
    "duplicate_cols = [col for col in set(all_cols) if all_cols.count(col) > 1]\n",
    "\n",
    "# For each duplicate column, merge them intelligently (keep non-NaN values)\n",
    "for col_name in duplicate_cols:\n",
    "    col_indices = [i for i, c in enumerate(dsc_sets.columns) if c == col_name]\n",
    "    if len(col_indices) > 1:\n",
    "        # Get all columns with this name\n",
    "        cols_data = [dsc_sets.iloc[:, idx] for idx in col_indices]\n",
    "        \n",
    "        # Merge: start with first column, fill NaN with values from other columns\n",
    "        merged_col = cols_data[0].copy()\n",
    "        for other_col in cols_data[1:]:\n",
    "            # Fill NaN values with non-NaN values from other column\n",
    "            mask = merged_col.isna() & other_col.notna()\n",
    "            merged_col[mask] = other_col[mask]\n",
    "        \n",
    "        # Replace first occurrence with merged data\n",
    "        dsc_sets.iloc[:, col_indices[0]] = merged_col\n",
    "        \n",
    "        # Drop all other occurrences\n",
    "        cols_to_drop = [dsc_sets.columns[i] for i in col_indices[1:]]\n",
    "        dsc_sets = dsc_sets.drop(columns=cols_to_drop, errors='ignore')\n",
    "        \n",
    "        # Rename to ensure consistency (in case pandas kept different names)\n",
    "        dsc_sets = dsc_sets.rename(columns={dsc_sets.columns[col_indices[0]]: col_name})\n",
    "\n",
    "# Now remove any remaining duplicates by position\n",
    "dsc_sets = dsc_sets.loc[:, ~dsc_sets.columns.duplicated(keep='first')]\n",
    "\n",
    "# Verify duplicates are removed\n",
    "remaining_duplicates = [col for col in set(dsc_sets.columns) if list(dsc_sets.columns).count(col) > 1]\n",
    "if remaining_duplicates:\n",
    "    # Force remove by recreating dataframe with unique columns\n",
    "    dsc_sets = dsc_sets.iloc[:, ~dsc_sets.columns.duplicated(keep='first')]\n",
    "else:\n",
    "\n",
    "# NOW extract numeric grade from Avg_Grade_Received (after duplicates are removed)\n",
    "if \"Avg_Grade_Received\" in dsc_sets.columns:\n",
    "    # Get the column as a Series (should be unique now)\n",
    "    col_idx = list(dsc_sets.columns).index(\"Avg_Grade_Received\")\n",
    "    col_series = dsc_sets.iloc[:, col_idx]\n",
    "    \n",
    "    # Check if column contains strings (like \"3.93 (A-)\") or is already numeric\n",
    "    if col_series.dtype == 'object':\n",
    "        # Extract numeric GPA value from strings like \"3.93 (A-)\" or \"3.93\"\n",
    "        # Pattern matches: start of string, digits, optional decimal point and more digits\n",
    "        extracted = col_series.astype(str).str.extract(r'^(\\d+\\.?\\d*)', expand=False)\n",
    "        dsc_sets.iloc[:, col_idx] = pd.to_numeric(extracted, errors='coerce')\n",
    "        non_null_count = dsc_sets.iloc[:, col_idx].notna().sum()\n",
    "    else:\n",
    "        # Already numeric, just ensure it's the right type\n",
    "        dsc_sets.iloc[:, col_idx] = pd.to_numeric(col_series, errors='coerce')\n",
    "else:\n",
    "    # Try alternative names\n",
    "    for col in dsc_sets.columns:\n",
    "        if 'Grade' in col and 'Received' in col:\n",
    "            col_idx = list(dsc_sets.columns).index(col)\n",
    "            col_series = dsc_sets.iloc[:, col_idx]\n",
    "            if col_series.dtype == 'object':\n",
    "                # Extract numeric values from strings like \"3.93 (A-)\"\n",
    "                extracted = col_series.astype(str).str.extract(r'^(\\d+\\.?\\d*)', expand=False)\n",
    "                dsc_sets.iloc[:, col_idx] = pd.to_numeric(extracted, errors='coerce')\n",
    "            else:\n",
    "                dsc_sets.iloc[:, col_idx] = pd.to_numeric(col_series, errors='coerce')\n",
    "            # Rename to Avg_Grade_Received\n",
    "            dsc_sets = dsc_sets.rename(columns={col: \"Avg_Grade_Received\"})\n",
    "            break\n",
    "\n",
    "# Extract course number and quarter\n",
    "dsc_sets['course_number'] = dsc_sets['course_title'].apply(extract_course_number)\n",
    "dsc_sets = dsc_sets[dsc_sets['course_number'].notna()]\n",
    "dsc_sets['quarter'] = dsc_sets['Term'].apply(standardize_term)\n",
    "\n",
    "\n",
    "# Safe function to get non-null count\n",
    "def safe_nonnull_count(df, col_name):\n",
    "    \"\"\"Safely count non-null values in a column\"\"\"\n",
    "    if col_name not in df.columns:\n",
    "        return None\n",
    "    try:\n",
    "        col_idx = list(df.columns).index(col_name)\n",
    "        col_data = df.iloc[:, col_idx]\n",
    "        count = col_data.notna().sum()\n",
    "        return int(count) if not isinstance(count, pd.Series) else int(count.iloc[0]) if len(count) > 0 else 0\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "hours_count = safe_nonnull_count(dsc_sets, 'Avg_Hours_Worked')\n",
    "grade_count = safe_nonnull_count(dsc_sets, 'Avg_Grade_Received')\n",
    "learning_count = safe_nonnull_count(dsc_sets, 'Learning_Average')\n",
    "structure_count = safe_nonnull_count(dsc_sets, 'Structure_Average')\n",
    "environment_count = safe_nonnull_count(dsc_sets, 'Environment_Average')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8653d312",
   "metadata": {},
   "source": [
    "## Step 3: Aggregate SET Data and Create Feature Set\n",
    "\n",
    "We'll aggregate SET data by course and quarter to create our feature set. The dataset will include:\n",
    "- **SET features**: Learning Average, Structure Average, Environment Average, Study Hours, Grades (ALL features from SET data only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa7ca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Aggregate SET data by course and quarter\n",
    "\n",
    "# Find the actual column names after cleaning\n",
    "hours_col_set = None\n",
    "grade_col_set = 'Avg_Grade_Received'  # We created this\n",
    "learning_col_set = None\n",
    "structure_col_set = None\n",
    "environment_col_set = None\n",
    "\n",
    "for col in dsc_sets.columns:\n",
    "    if 'Hours' in col or 'hours' in col.lower():\n",
    "        hours_col_set = col\n",
    "    if 'Learning' in col and 'Average' in col:\n",
    "        learning_col_set = col\n",
    "    if 'Structure' in col and 'Average' in col:\n",
    "        structure_col_set = col\n",
    "    if 'Environment' in col and 'Average' in col:\n",
    "        environment_col_set = col\n",
    "\n",
    "\n",
    "# Convert columns to numeric before aggregation (handle any string values)\n",
    "numeric_cols = []\n",
    "cols_to_convert = []\n",
    "\n",
    "if hours_col_set and hours_col_set in dsc_sets.columns:\n",
    "    cols_to_convert.append(hours_col_set)\n",
    "if grade_col_set and grade_col_set in dsc_sets.columns:\n",
    "    cols_to_convert.append(grade_col_set)\n",
    "if learning_col_set and learning_col_set in dsc_sets.columns:\n",
    "    cols_to_convert.append(learning_col_set)\n",
    "if structure_col_set and structure_col_set in dsc_sets.columns:\n",
    "    cols_to_convert.append(structure_col_set)\n",
    "if environment_col_set and environment_col_set in dsc_sets.columns:\n",
    "    cols_to_convert.append(environment_col_set)\n",
    "\n",
    "# Convert each column to numeric - handle duplicates and ensure Series\n",
    "for col in cols_to_convert:\n",
    "    if col in dsc_sets.columns:\n",
    "        try:\n",
    "            # Check if column exists and get column index\n",
    "            col_indices = [i for i, c in enumerate(dsc_sets.columns) if c == col]\n",
    "            if len(col_indices) > 0:\n",
    "                # Use iloc to get the first occurrence as a Series\n",
    "                col_idx = col_indices[0]\n",
    "                col_series = dsc_sets.iloc[:, col_idx]\n",
    "                \n",
    "                # Special handling for grade column (may contain strings like \"3.93 (A-)\")\n",
    "                if col == grade_col_set and col_series.dtype == 'object':\n",
    "                    # Extract numeric GPA from strings like \"3.93 (A-)\" or \"3.93\"\n",
    "                    # Try to extract the first numeric value (GPA)\n",
    "                    extracted = col_series.astype(str).str.extract(r'^(\\d+\\.?\\d*)', expand=False)\n",
    "                    dsc_sets.iloc[:, col_idx] = pd.to_numeric(extracted, errors='coerce')\n",
    "                    non_null_after = dsc_sets.iloc[:, col_idx].notna().sum()\n",
    "                else:\n",
    "                    # Convert to numeric (handles already-numeric or other string formats)\n",
    "                    dsc_sets.iloc[:, col_idx] = pd.to_numeric(col_series, errors='coerce')\n",
    "                \n",
    "                numeric_cols.append(col)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "\n",
    "# Build aggregation dictionary (only for numeric columns with actual data)\n",
    "agg_dict_set = {}\n",
    "cols_to_check = [\n",
    "    (hours_col_set, 'study_hours'),\n",
    "    (grade_col_set, 'avg_grade'),\n",
    "    (learning_col_set, 'learning_avg'),\n",
    "    (structure_col_set, 'structure_avg'),\n",
    "    (environment_col_set, 'environment_avg')\n",
    "]\n",
    "\n",
    "for col_name, target_name in cols_to_check:\n",
    "    if col_name and col_name in numeric_cols:\n",
    "        # Check if column has any non-null values\n",
    "        col_idx = list(dsc_sets.columns).index(col_name) if col_name in dsc_sets.columns else None\n",
    "        if col_idx is not None:\n",
    "            col_data = dsc_sets.iloc[:, col_idx]\n",
    "            non_null_count = col_data.notna().sum()\n",
    "            if non_null_count > 0:\n",
    "                agg_dict_set[col_name] = 'mean'\n",
    "            else:\n",
    "\n",
    "\n",
    "if len(agg_dict_set) > 0:\n",
    "    # Only aggregate numeric columns\n",
    "    merged_df = dsc_sets.groupby(['course_number', 'quarter'])[list(agg_dict_set.keys())].mean().reset_index()\n",
    "    \n",
    "    # Rename columns\n",
    "    rename_dict_set = {}\n",
    "    if hours_col_set and hours_col_set in merged_df.columns:\n",
    "        rename_dict_set[hours_col_set] = 'study_hours'\n",
    "    if grade_col_set and grade_col_set in merged_df.columns:\n",
    "        rename_dict_set[grade_col_set] = 'avg_grade'\n",
    "    if learning_col_set and learning_col_set in merged_df.columns:\n",
    "        rename_dict_set[learning_col_set] = 'learning_avg'\n",
    "    if structure_col_set and structure_col_set in merged_df.columns:\n",
    "        rename_dict_set[structure_col_set] = 'structure_avg'\n",
    "    if environment_col_set and environment_col_set in merged_df.columns:\n",
    "        rename_dict_set[environment_col_set] = 'environment_avg'\n",
    "    \n",
    "    merged_df = merged_df.rename(columns=rename_dict_set)\n",
    "else:\n",
    "    merged_df = pd.DataFrame(columns=['course_number', 'quarter', 'study_hours', 'avg_grade',\n",
    "                                     'learning_avg', 'structure_avg', 'environment_avg'])\n",
    "\n",
    "\n",
    "# Check data completeness with safe column access\n",
    "\n",
    "def safe_count(df, col_name):\n",
    "    \"\"\"Safely count non-null values in a column, handling duplicates\"\"\"\n",
    "    if col_name not in df.columns:\n",
    "        return None\n",
    "    try:\n",
    "        # Get column by index to avoid duplicate issues\n",
    "        col_indices = [i for i, c in enumerate(df.columns) if c == col_name]\n",
    "        if len(col_indices) > 0:\n",
    "            col_idx = col_indices[0]\n",
    "            col_data = df.iloc[:, col_idx]\n",
    "            count = col_data.notna().sum()\n",
    "            # Ensure it's a scalar\n",
    "            if isinstance(count, pd.Series):\n",
    "                count = count.iloc[0] if len(count) > 0 else 0\n",
    "            return int(count)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "study_hours_count = safe_count(merged_df, 'study_hours')\n",
    "if study_hours_count is not None:\n",
    "else:\n",
    "\n",
    "avg_grade_count = safe_count(merged_df, 'avg_grade')\n",
    "if avg_grade_count is not None:\n",
    "else:\n",
    "\n",
    "learning_count = safe_count(merged_df, 'learning_avg')\n",
    "if learning_count is not None:\n",
    "else:\n",
    "\n",
    "structure_count = safe_count(merged_df, 'structure_avg')\n",
    "if structure_count is not None:\n",
    "else:\n",
    "\n",
    "environment_count = safe_count(merged_df, 'environment_avg')\n",
    "if environment_count is not None:\n",
    "else:\n",
    "\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7382199",
   "metadata": {},
   "source": [
    "## Step 4: Manually Define Difficulty Labels (Ground Truth)\n",
    "\n",
    "**Key Decision: Choosing the Feature for Manual Difficulty Definition**\n",
    "\n",
    "We will use **Study Hours** (Avg_Hours_Worked) from SET data as our manual difficulty definition because:\n",
    "\n",
    "1. **Direct Workload Indicator**: Study hours directly reflect the time commitment required, which correlates with difficulty\n",
    "2. **Objective Measure**: Based on actual reported hours, not subjective ratings\n",
    "3. **Interpretable**: Higher hours = more difficult/time-consuming course\n",
    "4. **Available in SET Data**: We have complete SET data with study hours for all courses\n",
    "\n",
    "**Definition:**\n",
    "- **Difficult (1)**: Study hours ≥ median (courses requiring more time commitment)\n",
    "- **Easy (0)**: Study hours < median (courses requiring less time commitment)\n",
    "\n",
    "This creates a balanced binary classification problem based on workload/difficulty.\n",
    "\n",
    "**Note:** All features (study hours, learning, structure, environment averages, grades) come from SET data only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3107be02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Manually define difficulty based on multiple SET features\n",
    "\n",
    "# Filter to rows with all required features\n",
    "required_features = ['study_hours', 'learning_avg', 'structure_avg', 'environment_avg']\n",
    "df_with_features = merged_df[merged_df[required_features].notna().all(axis=1)].copy()\n",
    "\n",
    "if len(df_with_features) == 0:\n",
    "    # Use courses with at least study_hours\n",
    "    df_with_features = merged_df[merged_df['study_hours'].notna()].copy()\n",
    "\n",
    "# Create composite difficulty score\n",
    "# Normalize each feature to [0, 1] range, then combine\n",
    "\n",
    "# Initialize difficulty components\n",
    "difficulty_components = {}\n",
    "\n",
    "# 1. Study hours: Higher = more difficult (normalize to [0,1])\n",
    "if 'study_hours' in df_with_features.columns and df_with_features['study_hours'].notna().sum() > 0:\n",
    "    hours_min = df_with_features['study_hours'].min()\n",
    "    hours_max = df_with_features['study_hours'].max()\n",
    "    if hours_max > hours_min:\n",
    "        difficulty_components['study_hours'] = (df_with_features['study_hours'] - hours_min) / (hours_max - hours_min)\n",
    "    else:\n",
    "        difficulty_components['study_hours'] = pd.Series(0.5, index=df_with_features.index)\n",
    "\n",
    "# 2. Learning average: Lower = more difficult (invert and normalize)\n",
    "if 'learning_avg' in df_with_features.columns and df_with_features['learning_avg'].notna().sum() > 0:\n",
    "    learning_min = df_with_features['learning_avg'].min()\n",
    "    learning_max = df_with_features['learning_avg'].max()\n",
    "    if learning_max > learning_min:\n",
    "        # Invert: lower learning_avg -> higher difficulty score\n",
    "        difficulty_components['learning_avg'] = 1 - ((df_with_features['learning_avg'] - learning_min) / (learning_max - learning_min))\n",
    "    else:\n",
    "        difficulty_components['learning_avg'] = pd.Series(0.5, index=df_with_features.index)\n",
    "\n",
    "# 3. Structure average: Lower = more difficult (invert and normalize)\n",
    "if 'structure_avg' in df_with_features.columns and df_with_features['structure_avg'].notna().sum() > 0:\n",
    "    structure_min = df_with_features['structure_avg'].min()\n",
    "    structure_max = df_with_features['structure_avg'].max()\n",
    "    if structure_max > structure_min:\n",
    "        # Invert: lower structure_avg -> higher difficulty score\n",
    "        difficulty_components['structure_avg'] = 1 - ((df_with_features['structure_avg'] - structure_min) / (structure_max - structure_min))\n",
    "    else:\n",
    "        difficulty_components['structure_avg'] = pd.Series(0.5, index=df_with_features.index)\n",
    "\n",
    "# 4. Environment average: Lower = more difficult (invert and normalize)\n",
    "if 'environment_avg' in df_with_features.columns and df_with_features['environment_avg'].notna().sum() > 0:\n",
    "    env_min = df_with_features['environment_avg'].min()\n",
    "    env_max = df_with_features['environment_avg'].max()\n",
    "    if env_max > env_min:\n",
    "        # Invert: lower environment_avg -> higher difficulty score\n",
    "        difficulty_components['environment_avg'] = 1 - ((df_with_features['environment_avg'] - env_min) / (env_max - env_min))\n",
    "    else:\n",
    "        difficulty_components['environment_avg'] = pd.Series(0.5, index=df_with_features.index)\n",
    "\n",
    "# Combine all components into composite difficulty score (equal weights)\n",
    "if len(difficulty_components) > 0:\n",
    "    # Average all available components\n",
    "    composite_scores = pd.DataFrame(difficulty_components).mean(axis=1)\n",
    "    df_with_features['difficulty_score'] = composite_scores\n",
    "    \n",
    "    \n",
    "    # Calculate median of composite score\n",
    "    median_score = composite_scores.median()\n",
    "    \n",
    "    # Define difficulty labels based on median split of composite score\n",
    "    # Difficult = 1 (composite score >= median)\n",
    "    # Easy = 0 (composite score < median)\n",
    "    df_with_features['difficulty_manual'] = (df_with_features['difficulty_score'] >= median_score).astype(int)\n",
    "    \n",
    "    # Create individual difficulty labels for each feature based on median split\n",
    "    \n",
    "    # 1. Study hours: Higher = more difficult\n",
    "    if 'study_hours' in df_with_features.columns and df_with_features['study_hours'].notna().sum() > 0:\n",
    "        median_hours = df_with_features['study_hours'].median()\n",
    "        df_with_features['difficulty_study_hours'] = (df_with_features['study_hours'] >= median_hours).astype(int)\n",
    "    \n",
    "    # 2. Average grade: Lower = more difficult (invert)\n",
    "    if 'avg_grade' in df_with_features.columns and df_with_features['avg_grade'].notna().sum() > 0:\n",
    "        median_grade = df_with_features['avg_grade'].median()\n",
    "        df_with_features['difficulty_avg_grade'] = (df_with_features['avg_grade'] < median_grade).astype(int)\n",
    "    \n",
    "    # 3. Learning average: Lower = more difficult (invert)\n",
    "    if 'learning_avg' in df_with_features.columns and df_with_features['learning_avg'].notna().sum() > 0:\n",
    "        median_learning = df_with_features['learning_avg'].median()\n",
    "        df_with_features['difficulty_learning_avg'] = (df_with_features['learning_avg'] < median_learning).astype(int)\n",
    "    \n",
    "    # 4. Structure average: Lower = more difficult (invert)\n",
    "    if 'structure_avg' in df_with_features.columns and df_with_features['structure_avg'].notna().sum() > 0:\n",
    "        median_structure = df_with_features['structure_avg'].median()\n",
    "        df_with_features['difficulty_structure_avg'] = (df_with_features['structure_avg'] < median_structure).astype(int)\n",
    "    \n",
    "    # 5. Environment average: Lower = more difficult (invert)\n",
    "    if 'environment_avg' in df_with_features.columns and df_with_features['environment_avg'].notna().sum() > 0:\n",
    "        median_environment = df_with_features['environment_avg'].median()\n",
    "        df_with_features['difficulty_environment_avg'] = (df_with_features['environment_avg'] < median_environment).astype(int)\n",
    "    \n",
    "else:\n",
    "    df_with_features['difficulty_score'] = 0.5\n",
    "    df_with_features['difficulty_manual'] = 0\n",
    "\n",
    "\n",
    "# Show statistics by difficulty category\n",
    "\n",
    "# Build aggregation dictionary with available features\n",
    "agg_dict = {\n",
    "    'difficulty_score': ['mean', 'std', 'min', 'max'],\n",
    "    'course_number': ['mean', 'std', 'min', 'max']\n",
    "}\n",
    "\n",
    "# Add features if they exist\n",
    "if 'study_hours' in df_with_features.columns:\n",
    "    agg_dict['study_hours'] = ['mean', 'std', 'min', 'max']\n",
    "if 'avg_grade' in df_with_features.columns:\n",
    "    agg_dict['avg_grade'] = ['mean', 'std']\n",
    "if 'learning_avg' in df_with_features.columns:\n",
    "    agg_dict['learning_avg'] = ['mean', 'std']\n",
    "if 'structure_avg' in df_with_features.columns:\n",
    "    agg_dict['structure_avg'] = ['mean', 'std']\n",
    "if 'environment_avg' in df_with_features.columns:\n",
    "    agg_dict['environment_avg'] = ['mean', 'std']\n",
    "\n",
    "difficulty_stats = df_with_features.groupby('difficulty_manual').agg(agg_dict).round(4)\n",
    "\n",
    "# Visualize the distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Histogram of composite difficulty score by difficulty label\n",
    "if 'difficulty_score' in df_with_features.columns:\n",
    "    axes[0, 0].hist(df_with_features[df_with_features['difficulty_manual']==0]['difficulty_score'], \n",
    "                    bins=20, alpha=0.7, label='Easy (0)', color='green')\n",
    "    axes[0, 0].hist(df_with_features[df_with_features['difficulty_manual']==1]['difficulty_score'], \n",
    "                    bins=20, alpha=0.7, label='Difficult (1)', color='red')\n",
    "    if 'difficulty_score' in df_with_features.columns:\n",
    "        median_score = df_with_features['difficulty_score'].median()\n",
    "        axes[0, 0].axvline(median_score, color='black', linestyle='--', linewidth=2, \n",
    "                          label=f'Median ({median_score:.3f})')\n",
    "    axes[0, 0].set_xlabel('Composite Difficulty Score')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].set_title('Distribution of Composite Difficulty Score')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Box plot comparing composite difficulty score\n",
    "if 'difficulty_score' in df_with_features.columns:\n",
    "    box_data = [\n",
    "        df_with_features[df_with_features['difficulty_manual']==0]['difficulty_score'].values,\n",
    "        df_with_features[df_with_features['difficulty_manual']==1]['difficulty_score'].values\n",
    "    ]\n",
    "    axes[0, 1].boxplot(box_data, labels=['Easy (0)', 'Difficult (1)'])\n",
    "    axes[0, 1].set_ylabel('Composite Difficulty Score')\n",
    "    axes[0, 1].set_title('Composite Difficulty Score by Category')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Study hours by difficulty (if available)\n",
    "if 'study_hours' in df_with_features.columns:\n",
    "    axes[1, 0].hist(df_with_features[df_with_features['difficulty_manual']==0]['study_hours'], \n",
    "                    bins=20, alpha=0.7, label='Easy (0)', color='green')\n",
    "    axes[1, 0].hist(df_with_features[df_with_features['difficulty_manual']==1]['study_hours'], \n",
    "                    bins=20, alpha=0.7, label='Difficult (1)', color='red')\n",
    "    axes[1, 0].set_xlabel('Study Hours per Week')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Study Hours Distribution by Difficulty Label')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Component contributions visualization\n",
    "if len(difficulty_components) > 0:\n",
    "    component_names = list(difficulty_components.keys())\n",
    "    easy_means = [df_with_features[df_with_features['difficulty_manual']==0][f'difficulty_score'].mean() \n",
    "                  if 'difficulty_score' in df_with_features.columns else 0.5]\n",
    "    difficult_means = [df_with_features[df_with_features['difficulty_manual']==1][f'difficulty_score'].mean() \n",
    "                       if 'difficulty_score' in df_with_features.columns else 0.5]\n",
    "    \n",
    "    # Show average component values by difficulty\n",
    "    if 'study_hours' in df_with_features.columns:\n",
    "        easy_hours = df_with_features[df_with_features['difficulty_manual']==0]['study_hours'].mean()\n",
    "        diff_hours = df_with_features[df_with_features['difficulty_manual']==1]['study_hours'].mean()\n",
    "        axes[1, 1].barh(['Easy', 'Difficult'], [easy_hours, diff_hours], \n",
    "                       color=['green', 'red'], alpha=0.7)\n",
    "        axes[1, 1].set_xlabel('Average Study Hours')\n",
    "        axes[1, 1].set_title('Average Study Hours by Difficulty Category')\n",
    "        axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'Study hours data\\nnot available', \n",
    "                       ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "        axes[1, 1].set_title('Component Analysis')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('difficulty_manual_definition.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "difficulty_cols = [col for col in df_with_features.columns if col.startswith('difficulty_')]\n",
    "for col in sorted(difficulty_cols):\n",
    "    if col not in ['difficulty_score', 'difficulty_manual']:\n",
    "        non_null = df_with_features[col].notna().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08fcf6b",
   "metadata": {},
   "source": [
    "## Step 5: Merge Utilization Rate and Run T-Tests\n",
    "\n",
    "We will:\n",
    "1. Load utilization_rate data from WebReg\n",
    "2. Merge with our difficulty-labeled dataset\n",
    "3. Run independent samples t-tests comparing utilization_rate between Easy (0) and Difficult (1) for each difficulty feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fa54d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Merge utilization_rate and run t-tests\n",
    "\n",
    "# Load WebReg data with utilization_rate\n",
    "webreg_data = pd.read_csv('webreg_data/results/webreg_processed_data.csv')\n",
    "\n",
    "# Prepare WebReg data for merging\n",
    "# Extract course number from course column (format: dsc_100 -> 100)\n",
    "def extract_course_num_from_webreg(course_str):\n",
    "    \"\"\"Extract course number from webreg course format (e.g., 'dsc_100' -> 100)\"\"\"\n",
    "    if pd.isna(course_str):\n",
    "        return None\n",
    "    import re\n",
    "    match = re.search(r'dsc[_\\s]*(\\d+)', str(course_str).lower())\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "webreg_data['course_number'] = webreg_data['course'].apply(extract_course_num_from_webreg)\n",
    "webreg_data['quarter'] = webreg_data['quarter'].str.lower()\n",
    "\n",
    "# Select only the columns we need\n",
    "webreg_merge = webreg_data[['course_number', 'quarter', 'utilization_rate']].copy()\n",
    "\n",
    "# Merge with df_with_features\n",
    "df_with_util = df_with_features.merge(\n",
    "    webreg_merge,\n",
    "    on=['course_number', 'quarter'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "\n",
    "# Get all difficulty feature columns\n",
    "difficulty_features = [col for col in df_with_util.columns if col.startswith('difficulty_') and col != 'difficulty_score']\n",
    "\n",
    "# Run independent samples t-tests for each difficulty feature\n",
    "\n",
    "t_test_results = []\n",
    "\n",
    "for feature in difficulty_features:\n",
    "    if feature not in df_with_util.columns:\n",
    "        continue\n",
    "    \n",
    "    # Get groups\n",
    "    easy_group = df_with_util[df_with_util[feature] == 0]['utilization_rate'].dropna()\n",
    "    difficult_group = df_with_util[df_with_util[feature] == 1]['utilization_rate'].dropna()\n",
    "    \n",
    "    if len(easy_group) == 0 or len(difficult_group) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Calculate means\n",
    "    easy_mean = easy_group.mean()\n",
    "    difficult_mean = difficult_group.mean()\n",
    "    mean_diff = abs(difficult_mean - easy_mean)\n",
    "    \n",
    "    # Run t-test\n",
    "    t_stat, p_value = ttest_ind(easy_group, difficult_group)\n",
    "    \n",
    "    # Store results\n",
    "    t_test_results.append({\n",
    "        'Difficulty_Feature': feature,\n",
    "        'Easy_Mean': easy_mean,\n",
    "        'Difficult_Mean': difficult_mean,\n",
    "        'Mean_Difference': mean_diff,\n",
    "        't_statistic': abs(t_stat),  # Use absolute value to match absolute mean difference\n",
    "        'p_value': p_value,\n",
    "        'n_Easy': len(easy_group),\n",
    "        'n_Difficult': len(difficult_group),\n",
    "        'Significant': 'Yes' if p_value < 0.05 else 'No'\n",
    "    })\n",
    "    \n",
    "\n",
    "# Create results DataFrame\n",
    "t_test_df = pd.DataFrame(t_test_results)\n",
    "\n",
    "\n",
    "# Visualize results\n",
    "if len(t_test_results) > 0:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, feature in enumerate(difficulty_features[:6]):  # Plot up to 6 features\n",
    "        if feature not in df_with_util.columns:\n",
    "            continue\n",
    "        \n",
    "        easy_group = df_with_util[df_with_util[feature] == 0]['utilization_rate'].dropna()\n",
    "        difficult_group = df_with_util[df_with_util[feature] == 1]['utilization_rate'].dropna()\n",
    "        \n",
    "        if len(easy_group) == 0 or len(difficult_group) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Box plot\n",
    "        box_data = [easy_group.values, difficult_group.values]\n",
    "        bp = axes[idx].boxplot(box_data, labels=['Easy (0)', 'Difficult (1)'], patch_artist=True)\n",
    "        bp['boxes'][0].set_facecolor('green')\n",
    "        bp['boxes'][0].set_alpha(0.7)\n",
    "        bp['boxes'][1].set_facecolor('red')\n",
    "        bp['boxes'][1].set_alpha(0.7)\n",
    "        \n",
    "        axes[idx].set_ylabel('Utilization Rate (%)')\n",
    "        axes[idx].set_title(f'{feature}\\np={t_test_df[t_test_df[\"Difficulty_Feature\"]==feature][\"p_value\"].values[0]:.4f}')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(difficulty_features), 6):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('t_test_utilization_by_difficulty.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3721f2ad",
   "metadata": {},
   "source": [
    "## Step 6: Two-Way ANOVA Tests\n",
    "\n",
    "We will run two-way ANOVA tests to examine interaction effects between:\n",
    "- **Composite difficulty score** (`difficulty_manual`) × **Individual features** (study_hours, avg_grade, learning_avg, structure_avg, environment_avg)\n",
    "\n",
    "**Purpose**: Test if the relationship between each feature and utilization_rate differs significantly between Easy (0) and Difficult (1) courses.\n",
    "\n",
    "**Hypotheses**:\n",
    "- **H₀**: No interaction effect (the relationship between feature and utilization_rate is the same for Easy and Difficult courses)\n",
    "- **H₁**: Significant interaction effect (the relationship differs between Easy and Difficult courses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05f15fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Two-way ANOVA tests\n",
    "\n",
    "# Features to test interactions with\n",
    "interaction_features = ['study_hours', 'avg_grade', 'learning_avg', 'structure_avg', 'environment_avg']\n",
    "\n",
    "anova_results = []\n",
    "\n",
    "for feature in interaction_features:\n",
    "    if feature not in df_with_util.columns:\n",
    "        continue\n",
    "    \n",
    "    # Filter to rows with all required data\n",
    "    test_data = df_with_util[[\n",
    "        'difficulty_manual', \n",
    "        feature, \n",
    "        'utilization_rate'\n",
    "    ]].dropna()\n",
    "    \n",
    "    if len(test_data) < 10:  # Need minimum data\n",
    "        continue\n",
    "    \n",
    "    # Convert difficulty_manual to categorical for ANOVA\n",
    "    test_data['difficulty_manual'] = test_data['difficulty_manual'].astype('category')\n",
    "    \n",
    "    # Create interaction term formula\n",
    "    # Format: utilization_rate ~ C(difficulty_manual) * feature\n",
    "    formula = f'utilization_rate ~ C(difficulty_manual) * {feature}'\n",
    "    \n",
    "    try:\n",
    "        # Fit two-way ANOVA model\n",
    "        model = ols(formula, data=test_data).fit()\n",
    "        anova_table = anova_lm(model, typ=2)\n",
    "        \n",
    "        # Extract interaction p-value\n",
    "        interaction_row = anova_table.loc[f'C(difficulty_manual):{feature}']\n",
    "        interaction_p = interaction_row['PR(>F)']\n",
    "        \n",
    "        # Extract main effects\n",
    "        main_effect_diff = anova_table.loc['C(difficulty_manual)', 'PR(>F)']\n",
    "        main_effect_feature = anova_table.loc[feature, 'PR(>F)']\n",
    "        \n",
    "        # Calculate means for each group\n",
    "        means = test_data.groupby('difficulty_manual').agg({\n",
    "            feature: 'mean',\n",
    "            'utilization_rate': 'mean'\n",
    "        })\n",
    "        \n",
    "        # Store results\n",
    "        anova_results.append({\n",
    "            'Feature': feature,\n",
    "            'Interaction_p_value': interaction_p,\n",
    "            'Main_Effect_Difficulty_p': main_effect_diff,\n",
    "            'Main_Effect_Feature_p': main_effect_feature,\n",
    "            'Easy_Mean_Feature': means.loc[0, feature],\n",
    "            'Difficult_Mean_Feature': means.loc[1, feature],\n",
    "            'Easy_Mean_Utilization': means.loc[0, 'utilization_rate'],\n",
    "            'Difficult_Mean_Utilization': means.loc[1, 'utilization_rate'],\n",
    "            'n': len(test_data),\n",
    "            'Interaction_Significant': 'Yes' if interaction_p < 0.05 else 'No'\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "# Create results DataFrame\n",
    "anova_df = pd.DataFrame(anova_results)\n",
    "\n",
    "if len(anova_df) > 0:\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}